# SEO

## 검색엔진 최적화 : Search Engine Optimization

> 구글에서 제공하고 있는 검색 엔진 최적화 기본 가이드를 참고하여 작성했습니다.

### 검색엔진 최적화(SEO) 란?

검색엔진이 보다 콘텐츠를 이해하고 제공할 수 있도록 도와주어 사이트를 개선하는 프로세스를 말한다.

### 1. 사이트 계층 구조 구성

- 디렉터리 구조를 간단하고 단순하게 만들 것

`../dir1/dir2/dir3/page.html` 와 같은 중첩이 깊어지는 경우 좋은 구성이라고 볼 수 없음

### 2. 명확하고 독창 타이틀 사용

- 메타 데이터 활용하기

meta description, keyword 등 적절하게 사용하면 좋다.

- 명확한 URL 구조로 개선

/123123 vs /car-seat-for-new-born-baby

- 하나의 URL로 통일 시키기
  - `<link rel="canonical" href="http://localhost/1.html"/>`
    통일할 사이트를 제외한 페이지에 이와 같이 링크에 canonical 속성을 지정해주면 사용자를 의도한 페이지로 이동 시킬 수 있음

  - 301 Redirection
    사용자가 사이트를 접속할 때, 내가 의도한 방향의 페이지로 이동할 수 있도록 한다. ( 서버 단에서 처리하는 방법 )

### 3. 현명한 a 태그 사용

- "여기를 클릭" 같은 일반적 앵커 텍스트 사용하지 말고 어떤 사이트인지 알 수 있도록 명시적인 키워드를 사용
- 주제와 연관이 없거나 연결된 페이지와 콘텐츠가 전혀 관련이 없는 텍스트 사용 X
- 링크를 쉽게 확인할 수 있도록 형식을 지정 → CSS 등을 이용해 일반 텍스트와 똑같이 보이게 하면 좋지 않음
- 내부 링크용 앵커 텍스트를 활용하면 사용자 뿐만 아니라 Google도 더 검색에 용이해짐

### 4. 이미지 최적화

- `picture` 태그는 반응형 이미지에 좋고, `loading="lazy"` 속성을 쓰면 빠른 페이지 로드 가능
- 이미지 관련 정보는 alt 속성 이용해 제공 할 수 있다. (대안 텍스트)
- 보편적인 이미지 파일 포맷 사용과 디렉토리(images) 설정 권장( images/123.jpg )

### 5. robots.txt 효과적으로 활용하기

- 사이트의 접근과 크롤링하는 것에 대해 어떻게 처리할지 내용이 담긴 `.txt` 파일
- 검색에 노출이 필요하지 않은 부분을 `robots.txt` 로 제어할 수 있다.

```xml
<!-- robots.txt 작성 예시 -->
User-agent: *
Disallow : /
Sitemap : /sitemap.xml
```

- 민감한 정보의 처리는 `robot.txt` 를 이용해 막는 것보다 암호화, 차단 등의 방법을 선택하는 것이 더 좋다.
- 사이트맵 활용 → 컴퓨터가 이해할 수 있는 xml의 사이트맵 파일을 생성

### 6. 페이지 순위 ( Page Rank )

- 어떤 사이트의 페이지를 노출할 것인지 순위를 결정하여 검색의 품질을 결정하는 개념
- 웹의 본질은 하이퍼텍스트(=링크)
- 많은 사이트가 링크되어 연결될 수록 그 사이트는 더 좋은 품질일 가능성이 높다고 평가를 한다.

이외에 더 많은 내용이 있지만 이번은 여기까지만 공부하는 걸로 .. 😅

## 참조

- [SEO 기본 가이드: 기본사항 | Google 검색 센터 | Google Developers](https://developers.google.com/search/docs/beginner/seo-starter-guide?hl=ko)
